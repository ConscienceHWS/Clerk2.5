#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PDFè½¬æ¢APIæµ‹è¯•è„šæœ¬
æµ‹è¯•ä¸Šä¼ æ–‡ä»¶ã€è½®è¯¢çŠ¶æ€ã€è·å–ç»“æœç­‰åŠŸèƒ½ï¼Œå¹¶ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
"""

import os
import sys
import time
import json
import requests
import statistics
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional
from dataclasses import dataclass, asdict
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# æµ‹è¯•é…ç½®
API_BASE_URL = "http://47.100.220.144:4214"
TEST_RUNS_PER_FILE = 3  # æ¯ä¸ªæ–‡ä»¶æµ‹è¯•æ¬¡æ•°
POLL_INTERVAL = 2  # è½®è¯¢é—´éš”ï¼ˆç§’ï¼‰
MAX_POLL_ATTEMPTS = 300  # æœ€å¤§è½®è¯¢æ¬¡æ•°ï¼ˆ300æ¬¡ * 2ç§’ = 10åˆ†é’Ÿè¶…æ—¶ï¼‰
SUPPORTED_EXTENSIONS = {'.pdf', '.png', '.jpg', '.jpeg'}

# æ–‡æ¡£ç±»å‹æ˜ å°„ï¼ˆæ ¹æ®æ–‡ä»¶åæ¨æ–­ï¼‰
FILE_TYPE_MAP = {
    'å™ªå£°': 'noiseRec',
    'ç”µç£': 'emRec',
    'å·¥å†µ': 'opStatus',
}


@dataclass
class TestResult:
    """å•æ¬¡æµ‹è¯•ç»“æœ"""
    file_name: str
    run_number: int
    task_id: Optional[str] = None
    upload_time: Optional[float] = None
    upload_status_code: Optional[int] = None
    upload_error: Optional[str] = None
    total_time: Optional[float] = None
    poll_count: int = 0
    final_status: Optional[str] = None
    json_response: Optional[Dict] = None
    json_file_path: Optional[str] = None  # ä¿å­˜çš„JSONæ–‡ä»¶è·¯å¾„
    error_message: Optional[str] = None
    success: bool = False


@dataclass
class ConcurrentTestResult:
    """å¹¶å‘æµ‹è¯•ç»“æœ"""
    file_name: str
    file_path: str
    task_id: Optional[str] = None
    upload_time: Optional[float] = None
    upload_status_code: Optional[int] = None
    total_time: Optional[float] = None
    poll_count: int = 0
    final_status: Optional[str] = None
    json_response: Optional[Dict] = None
    json_file_path: Optional[str] = None
    error_message: Optional[str] = None
    success: bool = False
    # å¯¹æ¯”æ•°æ®
    avg_sequential_time: float = 0.0  # ä¹‹å‰å•æ¬¡æµ‹è¯•çš„å¹³å‡è€—æ—¶
    time_difference: float = 0.0  # å¹¶å‘è€—æ—¶ä¸å¹³å‡è€—æ—¶çš„å·®å€¼
    time_difference_percent: float = 0.0  # è€—æ—¶å·®å¼‚ç™¾åˆ†æ¯”
    result_similarity: float = 0.0  # ä¸ä¹‹å‰ç»“æœçš„ä¸€è‡´æ€§ï¼ˆ0-1ï¼‰


@dataclass
class FileTestSummary:
    """å•ä¸ªæ–‡ä»¶çš„æµ‹è¯•æ±‡æ€»"""
    file_name: str
    file_path: str
    file_type: Optional[str] = None
    total_runs: int = 0
    successful_runs: int = 0
    failed_runs: int = 0
    avg_upload_time: float = 0.0
    avg_total_time: float = 0.0
    min_total_time: float = 0.0
    max_total_time: float = 0.0
    time_stability: float = 0.0  # æ—¶é—´ç¨³å®šåº¦ï¼ˆ1 - å˜å¼‚ç³»æ•°ï¼Œå€¼è¶Šå¤§è¶Šç¨³å®šï¼‰
    result_consistency: float = 0.0  # ç»“æœä¸€è‡´æ€§ï¼ˆJSONç»“æ„ä¸€è‡´æ€§ï¼Œ0-1ï¼‰
    results: List[TestResult] = None
    concurrent_result: Optional[ConcurrentTestResult] = None  # å¹¶å‘æµ‹è¯•ç»“æœ
    
    def __post_init__(self):
        if self.results is None:
            self.results = []


class APITester:
    """APIæµ‹è¯•å™¨"""
    
    def __init__(self, api_base_url: str, pdf_dir: str, runs_per_file: int = TEST_RUNS_PER_FILE, output_dir: Optional[str] = None):
        self.api_base_url = api_base_url.rstrip('/')
        self.pdf_dir = Path(pdf_dir)
        self.runs_per_file = runs_per_file
        self.test_results: List[FileTestSummary] = []
        self.session = requests.Session()
        self.session.timeout = 30
        
        # è®¾ç½®è¾“å‡ºç›®å½•ï¼ˆç”¨äºä¿å­˜JSONç»“æœï¼‰
        if output_dir is None:
            self.output_dir = project_root / "test" / "json_results"
        else:
            self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
    def infer_file_type(self, file_name: str) -> Optional[str]:
        """æ ¹æ®æ–‡ä»¶åæ¨æ–­æ–‡æ¡£ç±»å‹"""
        for keyword, file_type in FILE_TYPE_MAP.items():
            if keyword in file_name:
                return file_type
        return None
    
    def upload_file(self, file_path: Path, file_type: Optional[str] = None) -> Dict:
        """ä¸Šä¼ æ–‡ä»¶å¹¶è¿”å›å“åº”"""
        url = f"{self.api_base_url}/convert"
        
        with open(file_path, 'rb') as f:
            files = {'file': (file_path.name, f, self._get_content_type(file_path))}
            data = {}
            if file_type:
                data['type'] = file_type
            
            response = self.session.post(url, files=files, data=data)
            return {
                'status_code': response.status_code,
                'response': response.json() if response.status_code == 200 else None,
                'error': None if response.status_code == 200 else response.text
            }
    
    def _get_content_type(self, file_path: Path) -> str:
        """æ ¹æ®æ–‡ä»¶æ‰©å±•åè·å–Content-Type"""
        ext = file_path.suffix.lower()
        content_types = {
            '.pdf': 'application/pdf',
            '.png': 'image/png',
            '.jpg': 'image/jpeg',
            '.jpeg': 'image/jpeg',
        }
        return content_types.get(ext, 'application/octet-stream')
    
    def poll_task_status(self, task_id: str) -> Dict:
        """è½®è¯¢ä»»åŠ¡çŠ¶æ€ç›´åˆ°å®Œæˆ"""
        url = f"{self.api_base_url}/task/{task_id}/json"
        
        poll_count = 0
        while poll_count < MAX_POLL_ATTEMPTS:
            try:
                response = self.session.get(url)
                if response.status_code == 200:
                    # ä»»åŠ¡å®Œæˆï¼Œè¿”å›JSONæ•°æ®
                    return {
                        'status': 'completed',
                        'json_data': response.json(),
                        'poll_count': poll_count + 1
                    }
                elif response.status_code == 400:
                    # ä»»åŠ¡è¿˜åœ¨å¤„ç†ä¸­æˆ–å¤±è´¥
                    error_data = response.json()
                    if 'ä»»åŠ¡å°šæœªå®Œæˆ' in error_data.get('detail', ''):
                        # è¿˜åœ¨å¤„ç†ä¸­ï¼Œç»§ç»­è½®è¯¢
                        poll_count += 1
                        time.sleep(POLL_INTERVAL)
                        continue
                    else:
                        # ä»»åŠ¡å¤±è´¥
                        return {
                            'status': 'failed',
                            'error': error_data.get('detail', 'æœªçŸ¥é”™è¯¯'),
                            'poll_count': poll_count + 1
                        }
                else:
                    return {
                        'status': 'error',
                        'error': f'HTTP {response.status_code}: {response.text}',
                        'poll_count': poll_count + 1
                    }
            except requests.exceptions.RequestException as e:
                return {
                    'status': 'error',
                    'error': str(e),
                    'poll_count': poll_count + 1
                }
        
        # è¶…æ—¶
        return {
            'status': 'timeout',
            'error': f'è½®è¯¢è¶…æ—¶ï¼ˆè¶…è¿‡{MAX_POLL_ATTEMPTS * POLL_INTERVAL}ç§’ï¼‰',
            'poll_count': poll_count
        }
    
    def test_file(self, file_path: Path, run_number: int, file_type: Optional[str] = None) -> TestResult:
        """æµ‹è¯•å•ä¸ªæ–‡ä»¶çš„ä¸€æ¬¡è¿è¡Œ"""
        result = TestResult(
            file_name=file_path.name,
            run_number=run_number
        )
        
        print(f"  [è¿è¡Œ {run_number}] å¼€å§‹æµ‹è¯• {file_path.name}...")
        
        # æ­¥éª¤1: ä¸Šä¼ æ–‡ä»¶
        upload_start = time.time()
        try:
            upload_result = self.upload_file(file_path, file_type)
            result.upload_time = time.time() - upload_start
            result.upload_status_code = upload_result['status_code']
            
            if upload_result['status_code'] != 200:
                result.error_message = f"ä¸Šä¼ å¤±è´¥: {upload_result['error']}"
                result.upload_error = upload_result['error']
                print(f"    âŒ ä¸Šä¼ å¤±è´¥: {result.error_message}")
                return result
            
            task_id = upload_result['response']['task_id']
            result.task_id = task_id
            print(f"    âœ“ ä¸Šä¼ æˆåŠŸï¼Œtask_id: {task_id} (è€—æ—¶: {result.upload_time:.2f}s)")
            
        except Exception as e:
            result.upload_time = time.time() - upload_start
            result.error_message = f"ä¸Šä¼ å¼‚å¸¸: {str(e)}"
            print(f"    âŒ ä¸Šä¼ å¼‚å¸¸: {result.error_message}")
            return result
        
        # æ­¥éª¤2: è½®è¯¢çŠ¶æ€
        poll_start = time.time()
        poll_result = self.poll_task_status(task_id)
        result.poll_count = poll_result['poll_count']
        result.total_time = time.time() - upload_start  # æ€»æ—¶é—´åŒ…æ‹¬ä¸Šä¼ å’Œè½®è¯¢
        
        if poll_result['status'] == 'completed':
            result.final_status = 'completed'
            result.json_response = poll_result['json_data']
            result.success = True
            
            # ä¿å­˜JSONç»“æœåˆ°æ–‡ä»¶
            try:
                json_file_path = self._save_json_result(file_path, run_number, result.json_response, result.task_id)
                result.json_file_path = str(json_file_path)
                print(f"    âœ“ ä»»åŠ¡å®Œæˆ (æ€»è€—æ—¶: {result.total_time:.2f}s, è½®è¯¢æ¬¡æ•°: {result.poll_count})")
                print(f"    âœ“ JSONå·²ä¿å­˜: {json_file_path.name}")
            except Exception as e:
                print(f"    âš ï¸  ä»»åŠ¡å®Œæˆä½†ä¿å­˜JSONå¤±è´¥: {str(e)}")
        else:
            result.final_status = poll_result['status']
            result.error_message = poll_result.get('error', 'æœªçŸ¥é”™è¯¯')
            print(f"    âŒ ä»»åŠ¡å¤±è´¥: {result.error_message} (æ€»è€—æ—¶: {result.total_time:.2f}s)")
        
        return result
    
    def _save_json_result(self, file_path: Path, run_number: int, json_data: Dict, task_id: str) -> Path:
        """ä¿å­˜JSONç»“æœåˆ°æ–‡ä»¶"""
        # ç”Ÿæˆæ–‡ä»¶å: åŸæ–‡ä»¶å_è¿è¡Œæ¬¡æ•°_taskidå‰8ä½.json
        file_stem = file_path.stem
        task_id_short = task_id[:8] if task_id else "unknown"
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        json_filename = f"{file_stem}_run{run_number}_{task_id_short}_{timestamp}.json"
        json_file_path = self.output_dir / json_filename
        
        with open(json_file_path, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)
        
        return json_file_path
    
    def _calculate_time_stability(self, times: List[float]) -> float:
        """è®¡ç®—æ—¶é—´ç¨³å®šåº¦ï¼ˆ1 - å˜å¼‚ç³»æ•°ï¼Œå€¼è¶Šå¤§è¶Šç¨³å®šï¼‰"""
        if len(times) < 2:
            return 1.0 if len(times) == 1 else 0.0
        
        try:
            mean = statistics.mean(times)
            if mean == 0:
                return 0.0
            stdev = statistics.stdev(times)
            cv = stdev / mean  # å˜å¼‚ç³»æ•°
            stability = max(0.0, min(1.0, 1.0 - cv))  # è½¬æ¢ä¸ºç¨³å®šåº¦ï¼ŒèŒƒå›´0-1
            return stability
        except Exception:
            return 0.0
    
    def _calculate_result_consistency(self, json_responses: List[Dict]) -> float:
        """è®¡ç®—ç»“æœä¸€è‡´æ€§ï¼ˆJSONç»“æ„ä¸€è‡´æ€§ï¼Œ0-1ï¼‰"""
        if len(json_responses) < 2:
            return 1.0 if len(json_responses) == 1 else 0.0
        
        try:
            # æå–æ‰€æœ‰JSONçš„é”®ç»“æ„
            def get_keys_structure(obj, prefix=""):
                """é€’å½’è·å–æ‰€æœ‰é”®çš„è·¯å¾„"""
                keys = set()
                if isinstance(obj, dict):
                    for key, value in obj.items():
                        full_key = f"{prefix}.{key}" if prefix else key
                        keys.add(full_key)
                        if isinstance(value, (dict, list)):
                            keys.update(get_keys_structure(value, full_key))
                elif isinstance(obj, list) and len(obj) > 0:
                    # å¯¹äºåˆ—è¡¨ï¼Œæ£€æŸ¥ç¬¬ä¸€ä¸ªå…ƒç´ çš„é”®ç»“æ„
                    keys.update(get_keys_structure(obj[0], prefix))
                return keys
            
            structures = [get_keys_structure(j) for j in json_responses]
            
            if not structures:
                return 0.0
            
            # è®¡ç®—æ‰€æœ‰ç»“æ„çš„äº¤é›†å’Œå¹¶é›†
            common_keys = set.intersection(*structures) if structures else set()
            all_keys = set.union(*structures) if structures else set()
            
            if not all_keys:
                return 0.0
            
            # ä¸€è‡´æ€§ = å…±åŒé”®æ•° / æ‰€æœ‰é”®æ•°
            consistency = len(common_keys) / len(all_keys)
            return consistency
        except Exception:
            return 0.0
    
    def test_all_files(self):
        """æµ‹è¯•æ‰€æœ‰æ–‡ä»¶"""
        # è·å–æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶
        test_files = []
        for ext in SUPPORTED_EXTENSIONS:
            test_files.extend(self.pdf_dir.glob(f'*{ext}'))
            test_files.extend(self.pdf_dir.glob(f'*{ext.upper()}'))
        
        if not test_files:
            print(f"âŒ åœ¨ {self.pdf_dir} ç›®å½•ä¸‹æœªæ‰¾åˆ°å¯æµ‹è¯•çš„æ–‡ä»¶")
            return
        
        print(f"æ‰¾åˆ° {len(test_files)} ä¸ªæ–‡ä»¶ï¼Œæ¯ä¸ªæ–‡ä»¶æµ‹è¯• {self.runs_per_file} æ¬¡")
        print("=" * 80)
        
        for file_path in sorted(test_files):
            print(f"\nğŸ“„ æµ‹è¯•æ–‡ä»¶: {file_path.name}")
            print("-" * 80)
            
            # æ¨æ–­æ–‡ä»¶ç±»å‹
            file_type = self.infer_file_type(file_path.name)
            if file_type:
                print(f"  æ¨æ–­æ–‡æ¡£ç±»å‹: {file_type}")
            
            # åˆ›å»ºæ–‡ä»¶æµ‹è¯•æ±‡æ€»
            file_summary = FileTestSummary(
                file_name=file_path.name,
                file_path=str(file_path),
                file_type=file_type
            )
            
            # å¯¹æ¯ä¸ªæ–‡ä»¶æµ‹è¯•å¤šæ¬¡
            for run_num in range(1, self.runs_per_file + 1):
                result = self.test_file(file_path, run_num, file_type)
                file_summary.results.append(result)
                file_summary.total_runs += 1
                
                if result.success:
                    file_summary.successful_runs += 1
                else:
                    file_summary.failed_runs += 1
                
                # æ¯æ¬¡æµ‹è¯•ä¹‹é—´ç¨ä½œå»¶è¿Ÿ
                if run_num < self.runs_per_file:
                    time.sleep(1)
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            successful_times = [r.total_time for r in file_summary.results if r.success and r.total_time]
            if successful_times:
                file_summary.avg_total_time = sum(successful_times) / len(successful_times)
                file_summary.min_total_time = min(successful_times)
                file_summary.max_total_time = max(successful_times)
                # è®¡ç®—æ—¶é—´ç¨³å®šåº¦
                file_summary.time_stability = self._calculate_time_stability(successful_times)
            
            upload_times = [r.upload_time for r in file_summary.results if r.upload_time]
            if upload_times:
                file_summary.avg_upload_time = sum(upload_times) / len(upload_times)
            
            # è®¡ç®—ç»“æœä¸€è‡´æ€§
            successful_json_responses = [r.json_response for r in file_summary.results 
                                        if r.success and r.json_response is not None]
            if len(successful_json_responses) >= 2:
                file_summary.result_consistency = self._calculate_result_consistency(successful_json_responses)
            elif len(successful_json_responses) == 1:
                file_summary.result_consistency = 1.0  # åªæœ‰ä¸€æ¬¡æˆåŠŸï¼Œè®¤ä¸ºå®Œå…¨ä¸€è‡´
            
            self.test_results.append(file_summary)
            
            # æ‰“å°æ–‡ä»¶æµ‹è¯•æ±‡æ€»
            print(f"\n  æ–‡ä»¶æµ‹è¯•æ±‡æ€»:")
            print(f"    æˆåŠŸ: {file_summary.successful_runs}/{file_summary.total_runs}")
            print(f"    å¹³å‡è€—æ—¶: {file_summary.avg_total_time:.2f}s")
            print(f"    æœ€å¿«: {file_summary.min_total_time:.2f}s")
            print(f"    æœ€æ…¢: {file_summary.max_total_time:.2f}s")
            print(f"    æ—¶é—´ç¨³å®šåº¦: {file_summary.time_stability:.2%}")
            print(f"    ç»“æœä¸€è‡´æ€§: {file_summary.result_consistency:.2%}")
    
    def _test_file_concurrent(self, file_path: Path, file_type: Optional[str], file_summary: FileTestSummary) -> ConcurrentTestResult:
        """å¹¶å‘æµ‹è¯•å•ä¸ªæ–‡ä»¶ï¼ˆå†…éƒ¨æ–¹æ³•ï¼Œç”¨äºçº¿ç¨‹æ± ï¼‰"""
        result = ConcurrentTestResult(
            file_name=file_path.name,
            file_path=str(file_path)
        )
        
        upload_start = time.time()
        try:
            upload_result = self.upload_file(file_path, file_type)
            result.upload_time = time.time() - upload_start
            result.upload_status_code = upload_result.get('status_code')
            
            if upload_result['status_code'] != 200:
                result.error_message = f"ä¸Šä¼ å¤±è´¥: {upload_result.get('error', 'æœªçŸ¥é”™è¯¯')}"
                return result
            
            task_id = upload_result['response']['task_id']
            result.task_id = task_id
            
            # è½®è¯¢çŠ¶æ€
            poll_result = self.poll_task_status(task_id)
            result.poll_count = poll_result['poll_count']
            result.total_time = time.time() - upload_start
            
            if poll_result['status'] == 'completed':
                result.final_status = 'completed'
                result.json_response = poll_result['json_data']
                result.success = True
                
                # ä¿å­˜JSONç»“æœ
                try:
                    json_file_path = self._save_json_result(file_path, 0, result.json_response, result.task_id)
                    result.json_file_path = str(json_file_path)
                except Exception:
                    pass
            else:
                result.final_status = poll_result['status']
                result.error_message = poll_result.get('error', 'æœªçŸ¥é”™è¯¯')
                
        except Exception as e:
            result.total_time = time.time() - upload_start
            result.error_message = f"æµ‹è¯•å¼‚å¸¸: {str(e)}"
        
        return result
    
    def _compare_with_sequential(self, concurrent_result: ConcurrentTestResult, file_summary: FileTestSummary):
        """å¯¹æ¯”å¹¶å‘æµ‹è¯•ç»“æœä¸å•æ¬¡æµ‹è¯•ç»“æœ"""
        # å¯¹æ¯”è€—æ—¶
        if concurrent_result.success and concurrent_result.total_time and file_summary.avg_total_time > 0:
            concurrent_result.avg_sequential_time = file_summary.avg_total_time
            concurrent_result.time_difference = concurrent_result.total_time - file_summary.avg_total_time
            concurrent_result.time_difference_percent = (concurrent_result.time_difference / file_summary.avg_total_time) * 100
        
        # å¯¹æ¯”ç»“æœå†…å®¹
        if concurrent_result.success and concurrent_result.json_response:
            # ä¸ä¹‹å‰æ‰€æœ‰æˆåŠŸçš„JSONç»“æœå¯¹æ¯”
            successful_json_responses = [r.json_response for r in file_summary.results 
                                        if r.success and r.json_response is not None]
            if successful_json_responses:
                # è®¡ç®—ä¸ä¹‹å‰ç»“æœçš„å¹³å‡ä¸€è‡´æ€§
                similarities = []
                for prev_json in successful_json_responses:
                    similarity = self._calculate_json_similarity(concurrent_result.json_response, prev_json)
                    similarities.append(similarity)
                concurrent_result.result_similarity = sum(similarities) / len(similarities) if similarities else 0.0
            else:
                concurrent_result.result_similarity = 1.0  # æ²¡æœ‰ä¹‹å‰çš„ç»“æœï¼Œè®¤ä¸ºå®Œå…¨ä¸€è‡´
    
    def _calculate_json_similarity(self, json1: Dict, json2: Dict) -> float:
        """è®¡ç®—ä¸¤ä¸ªJSONçš„ç›¸ä¼¼åº¦ï¼ˆ0-1ï¼‰"""
        try:
            def get_keys_structure(obj, prefix=""):
                """é€’å½’è·å–æ‰€æœ‰é”®çš„è·¯å¾„"""
                keys = set()
                if isinstance(obj, dict):
                    for key, value in obj.items():
                        full_key = f"{prefix}.{key}" if prefix else key
                        keys.add(full_key)
                        if isinstance(value, (dict, list)):
                            keys.update(get_keys_structure(value, full_key))
                elif isinstance(obj, list) and len(obj) > 0:
                    keys.update(get_keys_structure(obj[0], prefix))
                return keys
            
            keys1 = get_keys_structure(json1)
            keys2 = get_keys_structure(json2)
            
            if not keys1 and not keys2:
                return 1.0
            if not keys1 or not keys2:
                return 0.0
            
            common_keys = keys1.intersection(keys2)
            all_keys = keys1.union(keys2)
            
            return len(common_keys) / len(all_keys) if all_keys else 0.0
        except Exception:
            return 0.0
    
    def test_concurrent(self):
        """å¹¶å‘æµ‹è¯•æ‰€æœ‰æ–‡ä»¶"""
        if not self.test_results:
            print("âš ï¸  æ²¡æœ‰æµ‹è¯•ç»“æœï¼Œè·³è¿‡å¹¶å‘æµ‹è¯•")
            return
        
        print(f"\n{'=' * 80}")
        print("ğŸš€ å¼€å§‹å¹¶å‘æµ‹è¯•æ‰€æœ‰æ–‡ä»¶")
        print(f"{'=' * 80}\n")
        
        # å‡†å¤‡æµ‹è¯•ä»»åŠ¡
        test_tasks = []
        for file_summary in self.test_results:
            file_path = Path(file_summary.file_path)
            if file_path.exists():
                test_tasks.append((file_path, file_summary.file_type, file_summary))
        
        if not test_tasks:
            print("âŒ æ²¡æœ‰å¯æµ‹è¯•çš„æ–‡ä»¶")
            return
        
        print(f"å‡†å¤‡å¹¶å‘æµ‹è¯• {len(test_tasks)} ä¸ªæ–‡ä»¶...\n")
        
        # å¹¶å‘æ‰§è¡Œæµ‹è¯•
        concurrent_start = time.time()
        results_dict = {}  # file_path -> (result, file_summary)
        
        with ThreadPoolExecutor(max_workers=len(test_tasks)) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_file = {
                executor.submit(self._test_file_concurrent, file_path, file_type, file_summary): 
                (file_path, file_summary)
                for file_path, file_type, file_summary in test_tasks
            }
            
            # æ”¶é›†ç»“æœ
            completed = 0
            for future in as_completed(future_to_file):
                file_path, file_summary = future_to_file[future]
                completed += 1
                try:
                    result = future.result()
                    results_dict[file_path] = (result, file_summary)
                    
                    if result.success:
                        print(f"  [{completed}/{len(test_tasks)}] âœ… {file_path.name} - å®Œæˆ (è€—æ—¶: {result.total_time:.2f}s)")
                    else:
                        print(f"  [{completed}/{len(test_tasks)}] âŒ {file_path.name} - å¤±è´¥: {result.error_message}")
                except Exception as e:
                    print(f"  [{completed}/{len(test_tasks)}] âŒ {file_path.name} - å¼‚å¸¸: {str(e)}")
                    result = ConcurrentTestResult(
                        file_name=file_path.name,
                        file_path=str(file_path),
                        error_message=f"æµ‹è¯•å¼‚å¸¸: {str(e)}"
                    )
                    results_dict[file_path] = (result, file_summary)
        
        concurrent_total_time = time.time() - concurrent_start
        
        # å¯¹æ¯”åˆ†æå¹¶ä¿å­˜ç»“æœ
        print(f"\nå¹¶å‘æµ‹è¯•æ€»è€—æ—¶: {concurrent_total_time:.2f}ç§’")
        print("å¼€å§‹å¯¹æ¯”åˆ†æ...\n")
        
        for file_path, (result, file_summary) in results_dict.items():
            self._compare_with_sequential(result, file_summary)
            file_summary.concurrent_result = result
            
            # æ‰“å°å¯¹æ¯”ç»“æœ
            if result.success:
                print(f"ğŸ“Š {file_path.name}:")
                print(f"   å¹¶å‘è€—æ—¶: {result.total_time:.2f}s")
                print(f"   å¹³å‡å•æ¬¡è€—æ—¶: {result.avg_sequential_time:.2f}s")
                if result.time_difference_percent != 0:
                    diff_str = f"{result.time_difference_percent:+.1f}%"
                    print(f"   è€—æ—¶å·®å¼‚: {diff_str}")
                print(f"   ç»“æœç›¸ä¼¼åº¦: {result.result_similarity:.2%}")
            else:
                print(f"âŒ {file_path.name}: {result.error_message}")
        
        print(f"\n{'=' * 80}")
        print("âœ… å¹¶å‘æµ‹è¯•å®Œæˆ")
        print(f"{'=' * 80}\n")
    
    def generate_report(self, output_file: Optional[str] = None):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = project_root / "test" / f"test_report_{timestamp}.md"
        
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        total_files = len(self.test_results)
        total_runs = sum(f.total_runs for f in self.test_results)
        total_successful = sum(f.successful_runs for f in self.test_results)
        total_failed = sum(f.failed_runs for f in self.test_results)
        success_rate = (total_successful / total_runs * 100) if total_runs > 0 else 0
        
        all_times = []
        for file_summary in self.test_results:
            all_times.extend([r.total_time for r in file_summary.results if r.success and r.total_time])
        
        avg_time = sum(all_times) / len(all_times) if all_times else 0
        min_time = min(all_times) if all_times else 0
        max_time = max(all_times) if all_times else 0
        
        # è®¡ç®—æ€»ä½“ç¨³å®šåº¦
        all_time_stabilities = [f.time_stability for f in self.test_results if f.time_stability > 0]
        avg_time_stability = sum(all_time_stabilities) / len(all_time_stabilities) if all_time_stabilities else 0.0
        
        all_result_consistencies = [f.result_consistency for f in self.test_results if f.result_consistency > 0]
        avg_result_consistency = sum(all_result_consistencies) / len(all_result_consistencies) if all_result_consistencies else 0.0
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        report_lines = [
            "# PDFè½¬æ¢APIæµ‹è¯•æŠ¥å‘Š",
            "",
            f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"**APIåœ°å€**: {self.api_base_url}",
            f"**æµ‹è¯•ç›®å½•**: {self.pdf_dir}",
            f"**æ¯ä¸ªæ–‡ä»¶æµ‹è¯•æ¬¡æ•°**: {self.runs_per_file}",
            f"**JSONç»“æœä¿å­˜ç›®å½•**: {self.output_dir}",
            "",
            "## æ€»ä½“ç»Ÿè®¡",
            "",
            f"- æµ‹è¯•æ–‡ä»¶æ•°: {total_files}",
            f"- æ€»æµ‹è¯•æ¬¡æ•°: {total_runs}",
            f"- æˆåŠŸæ¬¡æ•°: {total_successful}",
            f"- å¤±è´¥æ¬¡æ•°: {total_failed}",
            f"- æˆåŠŸç‡: {success_rate:.2f}%",
            f"- å¹³å‡è€—æ—¶: {avg_time:.2f}ç§’",
            f"- æœ€å¿«è€—æ—¶: {min_time:.2f}ç§’",
            f"- æœ€æ…¢è€—æ—¶: {max_time:.2f}ç§’",
            f"- å¹³å‡æ—¶é—´ç¨³å®šåº¦: {avg_time_stability:.2%}",
            f"- å¹³å‡ç»“æœä¸€è‡´æ€§: {avg_result_consistency:.2%}",
            "",
            "## è¯¦ç»†æµ‹è¯•ç»“æœ",
            "",
        ]
        
        # æ¯ä¸ªæ–‡ä»¶çš„è¯¦ç»†ç»“æœ
        for file_summary in self.test_results:
            report_lines.extend([
                f"### {file_summary.file_name}",
                "",
                f"- **æ–‡ä»¶è·¯å¾„**: `{file_summary.file_path}`",
                f"- **æ–‡æ¡£ç±»å‹**: {file_summary.file_type or 'è‡ªåŠ¨æ¨æ–­'}",
                f"- **æµ‹è¯•æ¬¡æ•°**: {file_summary.total_runs}",
                f"- **æˆåŠŸæ¬¡æ•°**: {file_summary.successful_runs}",
                f"- **å¤±è´¥æ¬¡æ•°**: {file_summary.failed_runs}",
                f"- **å¹³å‡è€—æ—¶**: {file_summary.avg_total_time:.2f}ç§’",
                f"- **æœ€å¿«è€—æ—¶**: {file_summary.min_total_time:.2f}ç§’",
                f"- **æœ€æ…¢è€—æ—¶**: {file_summary.max_total_time:.2f}ç§’",
                f"- **æ—¶é—´ç¨³å®šåº¦**: {file_summary.time_stability:.2%}",
                f"- **ç»“æœä¸€è‡´æ€§**: {file_summary.result_consistency:.2%}",
                "",
                "#### æ¯æ¬¡è¿è¡Œè¯¦æƒ…",
                "",
                "| è¿è¡Œ | ä»»åŠ¡ID | ä¸Šä¼ è€—æ—¶(s) | æ€»è€—æ—¶(s) | è½®è¯¢æ¬¡æ•° | çŠ¶æ€ | JSONæ–‡ä»¶ | é”™è¯¯ä¿¡æ¯ |",
                "|------|--------|-------------|-----------|----------|------|----------|----------|",
            ])
            
            for result in file_summary.results:
                task_id_short = result.task_id[:8] + "..." if result.task_id else "N/A"
                upload_time_str = f"{result.upload_time:.2f}" if result.upload_time else "N/A"
                total_time_str = f"{result.total_time:.2f}" if result.total_time else "N/A"
                status_emoji = "âœ…" if result.success else "âŒ"
                status_text = result.final_status or "unknown"
                
                # JSONæ–‡ä»¶è·¯å¾„
                if result.json_file_path:
                    json_file_name = Path(result.json_file_path).name
                    json_file_link = f"[{json_file_name}]({result.json_file_path})"
                else:
                    json_file_link = "-"
                
                error_text = result.error_message or "-"
                if len(error_text) > 50:
                    error_text = error_text[:47] + "..."
                
                report_lines.append(
                    f"| {result.run_number} | {task_id_short} | {upload_time_str} | "
                    f"{total_time_str} | {result.poll_count} | {status_emoji} {status_text} | {json_file_link} | {error_text} |"
                )
            
            # æ·»åŠ å¹¶å‘æµ‹è¯•å¯¹æ¯”
            if file_summary.concurrent_result:
                concurrent_result = file_summary.concurrent_result
                report_lines.extend([
                    "",
                    "#### å¹¶å‘æµ‹è¯•å¯¹æ¯”",
                    "",
                ])
                
                if concurrent_result.success:
                    report_lines.extend([
                        f"- **å¹¶å‘æµ‹è¯•è€—æ—¶**: {concurrent_result.total_time:.2f}ç§’",
                        f"- **å¹³å‡å•æ¬¡æµ‹è¯•è€—æ—¶**: {concurrent_result.avg_sequential_time:.2f}ç§’",
                        f"- **è€—æ—¶å·®å¼‚**: {concurrent_result.time_difference:+.2f}ç§’ ({concurrent_result.time_difference_percent:+.1f}%)",
                        f"- **ç»“æœç›¸ä¼¼åº¦**: {concurrent_result.result_similarity:.2%}",
                    ])
                    if concurrent_result.json_file_path:
                        json_file_name = Path(concurrent_result.json_file_path).name
                        json_file_link = f"[{json_file_name}]({concurrent_result.json_file_path})"
                        report_lines.append(f"- **å¹¶å‘æµ‹è¯•JSONæ–‡ä»¶**: {json_file_link}")
                else:
                    report_lines.append(f"- **å¹¶å‘æµ‹è¯•çŠ¶æ€**: âŒ å¤±è´¥ - {concurrent_result.error_message or 'æœªçŸ¥é”™è¯¯'}")
                
                report_lines.append("")
            
            report_lines.append("")
        
        # å¹¶å‘æµ‹è¯•æ±‡æ€»
        concurrent_results = [f.concurrent_result for f in self.test_results if f.concurrent_result]
        if concurrent_results:
            concurrent_successful = [r for r in concurrent_results if r.success]
            concurrent_failed = [r for r in concurrent_results if not r.success]
            
            report_lines.extend([
                "## å¹¶å‘æµ‹è¯•æ±‡æ€»",
                "",
                f"- **å¹¶å‘æµ‹è¯•æ–‡ä»¶æ•°**: {len(concurrent_results)}",
                f"- **å¹¶å‘æµ‹è¯•æˆåŠŸæ•°**: {len(concurrent_successful)}",
                f"- **å¹¶å‘æµ‹è¯•å¤±è´¥æ•°**: {len(concurrent_failed)}",
            ])
            
            if concurrent_successful:
                concurrent_times = [r.total_time for r in concurrent_successful if r.total_time]
                sequential_times = [r.avg_sequential_time for r in concurrent_successful if r.avg_sequential_time > 0]
                similarities = [r.result_similarity for r in concurrent_successful if r.result_similarity > 0]
                
                if concurrent_times:
                    total_concurrent_time = sum(concurrent_times)
                    total_sequential_time = sum(sequential_times) if sequential_times else 0
                    avg_concurrent_time = sum(concurrent_times) / len(concurrent_times)
                    avg_sequential_time = sum(sequential_times) / len(sequential_times) if sequential_times else 0
                    avg_similarity = sum(similarities) / len(similarities) if similarities else 0
                    
                    report_lines.extend([
                        "",
                        f"- **å¹¶å‘æµ‹è¯•æ€»è€—æ—¶**: {total_concurrent_time:.2f}ç§’",
                        f"- **å•æ¬¡æµ‹è¯•æ€»è€—æ—¶ï¼ˆä¼°ç®—ï¼‰**: {total_sequential_time:.2f}ç§’",
                        f"- **å¹³å‡å¹¶å‘è€—æ—¶**: {avg_concurrent_time:.2f}ç§’",
                        f"- **å¹³å‡å•æ¬¡è€—æ—¶**: {avg_sequential_time:.2f}ç§’",
                        f"- **å¹³å‡ç»“æœç›¸ä¼¼åº¦**: {avg_similarity:.2%}",
                    ])
                    
                    if total_sequential_time > 0:
                        time_saved = total_sequential_time - total_concurrent_time
                        time_saved_percent = (time_saved / total_sequential_time) * 100
                        report_lines.extend([
                            f"- **æ—¶é—´èŠ‚çœ**: {time_saved:.2f}ç§’ ({time_saved_percent:.1f}%)",
                        ])
            
            report_lines.extend([
                "",
                "### å¹¶å‘æµ‹è¯•è¯¦ç»†å¯¹æ¯”",
                "",
                "| æ–‡ä»¶å | å¹¶å‘è€—æ—¶(s) | å¹³å‡å•æ¬¡è€—æ—¶(s) | è€—æ—¶å·®å¼‚ | ç»“æœç›¸ä¼¼åº¦ | çŠ¶æ€ |",
                "|--------|-------------|----------------|----------|------------|------|",
            ])
            
            for file_summary in self.test_results:
                if file_summary.concurrent_result:
                    cr = file_summary.concurrent_result
                    if cr.success:
                        time_diff_str = f"{cr.time_difference:+.2f}s ({cr.time_difference_percent:+.1f}%)"
                        similarity_str = f"{cr.result_similarity:.2%}"
                        status_emoji = "âœ…"
                    else:
                        time_diff_str = "-"
                        similarity_str = "-"
                        status_emoji = "âŒ"
                    
                    concurrent_time_str = f"{cr.total_time:.2f}" if cr.total_time else "N/A"
                    sequential_time_str = f"{cr.avg_sequential_time:.2f}" if cr.avg_sequential_time > 0 else "N/A"
                    error_text = cr.error_message or "-"
                    if len(error_text) > 30:
                        error_text = error_text[:27] + "..."
                    
                    report_lines.append(
                        f"| {file_summary.file_name} | {concurrent_time_str} | {sequential_time_str} | "
                        f"{time_diff_str} | {similarity_str} | {status_emoji} {error_text} |"
                    )
            
            report_lines.append("")
        
        # é”™è¯¯æ±‡æ€»
        error_summary = defaultdict(int)
        for file_summary in self.test_results:
            for result in file_summary.results:
                if not result.success and result.error_message:
                    error_summary[result.error_message] += 1
        
        if error_summary:
            report_lines.extend([
                "## é”™è¯¯æ±‡æ€»",
                "",
                "| é”™è¯¯ä¿¡æ¯ | å‡ºç°æ¬¡æ•° |",
                "|----------|----------|",
            ])
            for error, count in sorted(error_summary.items(), key=lambda x: x[1], reverse=True):
                report_lines.append(f"| {error} | {count} |")
            report_lines.append("")
        
        # å†™å…¥æ–‡ä»¶
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))
        
        print(f"\n{'=' * 80}")
        print(f"âœ… æµ‹è¯•æŠ¥å‘Šå·²ç”Ÿæˆ: {output_path}")
        print(f"{'=' * 80}")
        
        return output_path


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='PDFè½¬æ¢APIæµ‹è¯•è„šæœ¬')
    parser.add_argument('--api-url', default=API_BASE_URL, help=f'APIåŸºç¡€URL (é»˜è®¤: {API_BASE_URL})')
    parser.add_argument('--pdf-dir', default=str(project_root / "test" / 'pdf'), help='PDFæ–‡ä»¶ç›®å½•')
    parser.add_argument('--runs', type=int, default=TEST_RUNS_PER_FILE, help=f'æ¯ä¸ªæ–‡ä»¶æµ‹è¯•æ¬¡æ•° (é»˜è®¤: {TEST_RUNS_PER_FILE})')
    parser.add_argument('--output', help='æµ‹è¯•æŠ¥å‘Šè¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--json-dir', help='JSONç»“æœä¿å­˜ç›®å½• (é»˜è®¤: test/json_results)')
    
    args = parser.parse_args()
    
    # åˆ›å»ºæµ‹è¯•å™¨
    tester = APITester(args.api_url, args.pdf_dir, runs_per_file=args.runs, output_dir=args.json_dir)
    
    # æ‰§è¡Œæµ‹è¯•
    try:
        tester.test_all_files()
        
        # æ‰€æœ‰å•æ¬¡æµ‹è¯•å®Œæˆåï¼Œè¿›è¡Œå¹¶å‘æµ‹è¯•
        if tester.test_results:
            tester.test_concurrent()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\n\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    
    # ç”ŸæˆæŠ¥å‘Š
    if tester.test_results:
        tester.generate_report(args.output)
    else:
        print("\nâš ï¸  æ²¡æœ‰æµ‹è¯•ç»“æœï¼Œè·³è¿‡æŠ¥å‘Šç”Ÿæˆ")


if __name__ == '__main__':
    main()

